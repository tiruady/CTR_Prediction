{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "m-UJGsbOvzGb",
        "outputId": "e3f2ff09-dd8b-4d75-f34b-ad948719781f"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 31kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.35.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.33.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=bc56a10a1f71ecc168a7b34c7e86e29e1041dfa4ff0f6d8b356a48444ac926ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 tensorflow-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ggl9XbYKydZ"
      },
      "source": [
        "##DICE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ5BYu_osQgt",
        "outputId": "0efcc577-d31f-4733-c777-017709d5e94e"
      },
      "source": [
        "#dice\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "def dice(_x, axis=-1, epsilon=0.000000001, name=''):\n",
        "  with tf.variable_scope(name_or_scope='', reuse=tf.AUTO_REUSE):\n",
        "    alphas = tf.get_variable('alpha'+name, _x.get_shape()[-1],                                  \n",
        "                         initializer=tf.constant_initializer(0.0),                         \n",
        "                         dtype=tf.float32)\n",
        "    beta = tf.get_variable('beta'+name, _x.get_shape()[-1],                                  \n",
        "                         initializer=tf.constant_initializer(0.0),                         \n",
        "                         dtype=tf.float32)\n",
        "  input_shape = list(_x.get_shape())\n",
        "\n",
        "  reduction_axes = list(range(len(input_shape)))\n",
        "  del reduction_axes[axis]\n",
        "  broadcast_shape = [1] * len(input_shape)\n",
        "  broadcast_shape[axis] = input_shape[axis]\n",
        "                                                                                                                                                                            \n",
        "  # case: train mode (uses stats of the current batch)\n",
        "  mean = tf.reduce_mean(_x, axis=reduction_axes)\n",
        "  brodcast_mean = tf.reshape(mean, broadcast_shape)\n",
        "  std = tf.reduce_mean(tf.square(_x - brodcast_mean) + epsilon, axis=reduction_axes)\n",
        "  std = tf.sqrt(std)\n",
        "  brodcast_std = tf.reshape(std, broadcast_shape)\n",
        "  x_normed = tf.layers.batch_normalization(_x, center=False, scale=False, name=name, reuse=tf.AUTO_REUSE)\n",
        "  # x_normed = (_x - brodcast_mean) / (brodcast_std + epsilon)\n",
        "  x_p = tf.sigmoid(beta * x_normed)\n",
        " \n",
        "  \n",
        "  return alphas * (1.0 - x_p) * _x + x_p * _x\n",
        "\n",
        "def parametric_relu(_x):\n",
        "  with tf.variable_scope(name_or_scope='', reuse=tf.AUTO_REUSE):\n",
        "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],\n",
        "                         initializer=tf.constant_initializer(0.0),\n",
        "                         dtype=tf.float32)\n",
        "  pos = tf.nn.relu(_x)\n",
        "  neg = alphas * (_x - abs(_x)) * 0.5\n",
        "\n",
        "  return pos + neg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2tdGsZyLM5j"
      },
      "source": [
        "#Iterators for the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9CBo9pesXMW"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class DataInput:\n",
        "  def __init__(self, data, batch_size):\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.data = data\n",
        "    self.epoch_size = len(self.data) // self.batch_size\n",
        "    if self.epoch_size * self.batch_size < len(self.data):\n",
        "      self.epoch_size += 1\n",
        "    self.i = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "\n",
        "    if self.i == self.epoch_size:\n",
        "      raise StopIteration\n",
        "\n",
        "    ts = self.data[self.i * self.batch_size : min((self.i+1) * self.batch_size,\n",
        "                                                  len(self.data))]\n",
        "    self.i += 1\n",
        "\n",
        "    u, i, y, sl = [], [], [], []\n",
        "    for t in ts:\n",
        "      u.append(t[0])\n",
        "      i.append(t[2])\n",
        "      y.append(t[3])\n",
        "      sl.append(len(t[1]))\n",
        "    max_sl = max(sl)\n",
        "\n",
        "    hist_i = np.zeros([len(ts), max_sl], np.int64)\n",
        "\n",
        "    k = 0\n",
        "    for t in ts:\n",
        "      for l in range(len(t[1])):\n",
        "        hist_i[k][l] = t[1][l]\n",
        "      k += 1\n",
        "\n",
        "    return self.i, (u, i, y, hist_i, sl)\n",
        "\n",
        "class DataInputTest:\n",
        "  def __init__(self, data, batch_size):\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.data = data\n",
        "    self.epoch_size = len(self.data) // self.batch_size\n",
        "    if self.epoch_size * self.batch_size < len(self.data):\n",
        "      self.epoch_size += 1\n",
        "    self.i = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "\n",
        "    if self.i == self.epoch_size:\n",
        "      raise StopIteration\n",
        "\n",
        "    ts = self.data[self.i * self.batch_size : min((self.i+1) * self.batch_size,\n",
        "                                                  len(self.data))]\n",
        "    self.i += 1\n",
        "\n",
        "    u, i, j, sl = [], [], [], []\n",
        "    for t in ts:\n",
        "      u.append(t[0])\n",
        "      i.append(t[2][0])\n",
        "      j.append(t[2][1])\n",
        "      sl.append(len(t[1]))\n",
        "    max_sl = max(sl)\n",
        "\n",
        "    hist_i = np.zeros([len(ts), max_sl], np.int64)\n",
        "\n",
        "    k = 0\n",
        "    for t in ts:\n",
        "      for l in range(len(t[1])):\n",
        "        hist_i[k][l] = t[1][l]\n",
        "      k += 1\n",
        "\n",
        "    return self.i, (u, i, j, hist_i, sl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMnVjBx2K2z2"
      },
      "source": [
        "###DIN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfhIRKZ8sh9B"
      },
      "source": [
        "#model\n",
        "class Model(object):\n",
        "\n",
        "  def __init__(self, user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num):\n",
        "\n",
        "    self.u = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.i = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.j = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.y = tf.placeholder(tf.float32, [None,]) # [B]\n",
        "    self.hist_i = tf.placeholder(tf.int32, [None, None]) # [B, T]\n",
        "    self.sl = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.lr = tf.placeholder(tf.float64, [])\n",
        "\n",
        "    hidden_units = 128\n",
        "\n",
        "    user_emb_w = tf.get_variable(\"user_emb_w\", [user_count, hidden_units])\n",
        "    item_emb_w = tf.get_variable(\"item_emb_w\", [item_count, hidden_units // 2])\n",
        "    item_b = tf.get_variable(\"item_b\", [item_count],\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "    cate_emb_w = tf.get_variable(\"cate_emb_w\", [cate_count, hidden_units // 2])\n",
        "    cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\n",
        "\n",
        "    ic = tf.gather(cate_list, self.i)\n",
        "    i_emb = tf.concat(values = [\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, ic),\n",
        "        ], axis=1)\n",
        "    i_b = tf.gather(item_b, self.i)\n",
        "\n",
        "    jc = tf.gather(cate_list, self.j)\n",
        "    j_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.j),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, jc),\n",
        "        ], axis=1)\n",
        "    j_b = tf.gather(item_b, self.j)\n",
        "\n",
        "    hc = tf.gather(cate_list, self.hist_i)\n",
        "    h_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.hist_i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, hc),\n",
        "        ], axis=2)\n",
        "\n",
        "    hist_i =attention(i_emb, h_emb, self.sl)\n",
        "    #-- attention end ---\n",
        "    \n",
        "    hist_i = tf.layers.batch_normalization(inputs = hist_i)\n",
        "    hist_i = tf.reshape(hist_i, [-1, hidden_units], name='hist_bn')\n",
        "    hist_i = tf.layers.dense(hist_i, hidden_units, name='hist_fcn')\n",
        "\n",
        "    u_emb_i = hist_i\n",
        "    \n",
        "    hist_j =attention(j_emb, h_emb, self.sl)\n",
        "    #-- attention end ---\n",
        "    \n",
        "    # hist_j = tf.layers.batch_normalization(inputs = hist_j)\n",
        "    hist_j = tf.layers.batch_normalization(inputs = hist_j, reuse=True)\n",
        "    hist_j = tf.reshape(hist_j, [-1, hidden_units], name='hist_bn')\n",
        "    hist_j = tf.layers.dense(hist_j, hidden_units, name='hist_fcn', reuse=True)\n",
        "\n",
        "    u_emb_j = hist_j\n",
        "    print(u_emb_i.get_shape().as_list())\n",
        "    print(u_emb_j.get_shape().as_list())\n",
        "    print(i_emb.get_shape().as_list())\n",
        "    print(j_emb.get_shape().as_list())\n",
        "    #-- fcn begin -------\n",
        "    din_i = tf.concat([u_emb_i, i_emb, u_emb_i * i_emb], axis=-1)\n",
        "    din_i = tf.layers.batch_normalization(inputs=din_i, name='b1')\n",
        "    d_layer_1_i = tf.layers.dense(din_i, 80, activation=tf.nn.sigmoid, name='f1')\n",
        "    #if u want try dice change sigmoid to None and add dice layer like following two lines. You can also find model_dice.py in this folder.\n",
        "    # d_layer_1_i = tf.layers.dense(din_i, 80, activation=None, name='f1')\n",
        "    # d_layer_1_i = dice(d_layer_1_i, name='dice_1_i')\n",
        "    d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=tf.nn.sigmoid, name='f2')\n",
        "    # d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=None, name='f2')\n",
        "    # d_layer_2_i = dice(d_layer_2_i, name='dice_2_i')\n",
        "    d_layer_3_i = tf.layers.dense(d_layer_2_i, 1, activation=None, name='f3')\n",
        "    din_j = tf.concat([u_emb_j, j_emb, u_emb_j * j_emb], axis=-1)\n",
        "    din_j = tf.layers.batch_normalization(inputs=din_j, name='b1', reuse=True)\n",
        "    d_layer_1_j = tf.layers.dense(din_j, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
        "    # d_layer_1_j = tf.layers.dense(din_j, 80, activation=None, name='f1', reuse=True)\n",
        "    # d_layer_1_j = dice(d_layer_1_j, name='dice_1_j')\n",
        "    d_layer_2_j = tf.layers.dense(d_layer_1_j, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
        "    # d_layer_2_j = tf.layers.dense(d_layer_1_j, 40, activation=None, name='f2', reuse=True)\n",
        "    # d_layer_2_j = dice(d_layer_2_j, name='dice_2_j')\n",
        "    d_layer_3_j = tf.layers.dense(d_layer_2_j, 1, activation=None, name='f3', reuse=True)\n",
        "    d_layer_3_i = tf.reshape(d_layer_3_i, [-1])\n",
        "    d_layer_3_j = tf.reshape(d_layer_3_j, [-1])\n",
        "    x = i_b - j_b + d_layer_3_i - d_layer_3_j # [B]\n",
        "    self.logits = i_b + d_layer_3_i\n",
        "    \n",
        "    # prediciton for selected items\n",
        "    # logits for selected item:\n",
        "    item_emb_all = tf.concat([\n",
        "        item_emb_w,\n",
        "        tf.nn.embedding_lookup(cate_emb_w, cate_list)\n",
        "        ], axis=1)\n",
        "    item_emb_sub = item_emb_all[:predict_ads_num,:]\n",
        "    item_emb_sub = tf.expand_dims(item_emb_sub, 0)\n",
        "    item_emb_sub = tf.tile(item_emb_sub, [predict_batch_size, 1, 1])\n",
        "    hist_sub =attention_multi_items(item_emb_sub, h_emb, self.sl)\n",
        "    #-- attention end ---\n",
        "    \n",
        "    hist_sub = tf.layers.batch_normalization(inputs = hist_sub, name='hist_bn', reuse=tf.AUTO_REUSE)\n",
        "    # print hist_sub.get_shape().as_list() \n",
        "    hist_sub = tf.reshape(hist_sub, [-1, hidden_units])\n",
        "    hist_sub = tf.layers.dense(hist_sub, hidden_units, name='hist_fcn', reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    u_emb_sub = hist_sub\n",
        "    item_emb_sub = tf.reshape(item_emb_sub, [-1, hidden_units])\n",
        "    din_sub = tf.concat([u_emb_sub, item_emb_sub, u_emb_sub * item_emb_sub], axis=-1)\n",
        "    din_sub = tf.layers.batch_normalization(inputs=din_sub, name='b1', reuse=True)\n",
        "    d_layer_1_sub = tf.layers.dense(din_sub, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
        "    #d_layer_1_sub = dice(d_layer_1_sub, name='dice_1_sub')\n",
        "    d_layer_2_sub = tf.layers.dense(d_layer_1_sub, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
        "    #d_layer_2_sub = dice(d_layer_2_sub, name='dice_2_sub')\n",
        "    d_layer_3_sub = tf.layers.dense(d_layer_2_sub, 1, activation=None, name='f3', reuse=True)\n",
        "    d_layer_3_sub = tf.reshape(d_layer_3_sub, [-1, predict_ads_num])\n",
        "    self.logits_sub = tf.sigmoid(item_b[:predict_ads_num] + d_layer_3_sub)\n",
        "    self.logits_sub = tf.reshape(self.logits_sub, [-1, predict_ads_num, 1])\n",
        "    #-- fcn end -------\n",
        "\n",
        "    \n",
        "    self.mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
        "    self.score_i = tf.sigmoid(i_b + d_layer_3_i)\n",
        "    self.score_j = tf.sigmoid(j_b + d_layer_3_j)\n",
        "    self.score_i = tf.reshape(self.score_i, [-1, 1])\n",
        "    self.score_j = tf.reshape(self.score_j, [-1, 1])\n",
        "    self.p_and_n = tf.concat([self.score_i, self.score_j], axis=-1)\n",
        "    print(self.p_and_n.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Step variable\n",
        "    self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "    self.global_epoch_step = \\\n",
        "        tf.Variable(0, trainable=False, name='global_epoch_step')\n",
        "    self.global_epoch_step_op = \\\n",
        "        tf.assign(self.global_epoch_step, self.global_epoch_step+1)\n",
        "\n",
        "    self.loss = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=self.logits,\n",
        "            labels=self.y)\n",
        "        )\n",
        "\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
        "    gradients = tf.gradients(self.loss, trainable_params)\n",
        "    clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
        "    self.train_op = self.opt.apply_gradients(\n",
        "        zip(clip_gradients, trainable_params), global_step=self.global_step)\n",
        "\n",
        "\n",
        "  def train(self, sess, uij, l):\n",
        "    loss, _ = sess.run([self.loss, self.train_op], feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.y: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        self.lr: l,\n",
        "        })\n",
        "    return loss\n",
        "\n",
        "  def eval(self, sess, uij):\n",
        "    u_auc, socre_p_and_n = sess.run([self.mf_auc, self.p_and_n], feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.j: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        })\n",
        "    return u_auc, socre_p_and_n\n",
        "  \n",
        "  def test(self, sess, uij):\n",
        "    return sess.run(self.logits_sub, feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.j: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        })\n",
        "  \n",
        "\n",
        "  def save(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path=path)\n",
        "\n",
        "  def restore(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, save_path=path)\n",
        "\n",
        "def extract_axis_1(data, ind):\n",
        "  batch_range = tf.range(tf.shape(data)[0])\n",
        "  indices = tf.stack([batch_range, ind], axis=1)\n",
        "  res = tf.gather_nd(data, indices)\n",
        "  return res\n",
        "\n",
        "def attention(queries, keys, keys_length):\n",
        "  '''\n",
        "    queries:     [B, H]\n",
        "    keys:        [B, T, H]\n",
        "    keys_length: [B]\n",
        "  '''\n",
        "  queries_hidden_units = queries.get_shape().as_list()[-1]\n",
        "  queries = tf.tile(queries, [1, tf.shape(keys)[1]])\n",
        "  queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units])\n",
        "  din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
        "  d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])\n",
        "  outputs = d_layer_3_all \n",
        "  # Mask\n",
        "  key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])   # [B, T]\n",
        "  key_masks = tf.expand_dims(key_masks, 1) # [B, 1, T]\n",
        "  paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
        "  outputs = tf.where(key_masks, outputs, paddings)  # [B, 1, T]\n",
        "\n",
        "  # Scale\n",
        "  outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "  # Activation\n",
        "  outputs = tf.nn.softmax(outputs)  # [B, 1, T]\n",
        "\n",
        "  # Weighted sum\n",
        "  outputs = tf.matmul(outputs, keys)  # [B, 1, H]\n",
        "\n",
        "  return outputs\n",
        "\n",
        "def attention_multi_items(queries, keys, keys_length):\n",
        "  '''\n",
        "    queries:     [B, N, H] N is the number of ads\n",
        "    keys:        [B, T, H] \n",
        "    keys_length: [B]\n",
        "  '''\n",
        "  queries_hidden_units = queries.get_shape().as_list()[-1]\n",
        "  queries_nums = queries.get_shape().as_list()[1]\n",
        "  queries = tf.tile(queries, [1, 1, tf.shape(keys)[1]])\n",
        "  queries = tf.reshape(queries, [-1, queries_nums, tf.shape(keys)[1], queries_hidden_units]) # shape : [B, N, T, H]\n",
        "  max_len = tf.shape(keys)[1]\n",
        "  keys = tf.tile(keys, [1, queries_nums, 1])\n",
        "  keys = tf.reshape(keys, [-1, queries_nums, max_len, queries_hidden_units]) # shape : [B, N, T, H]\n",
        "  din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
        "  d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.reshape(d_layer_3_all, [-1, queries_nums, 1, max_len])\n",
        "  outputs = d_layer_3_all \n",
        "  # Mask\n",
        "  key_masks = tf.sequence_mask(keys_length, max_len)   # [B, T]\n",
        "  key_masks = tf.tile(key_masks, [1, queries_nums])\n",
        "  key_masks = tf.reshape(key_masks, [-1, queries_nums, 1, max_len]) # shape : [B, N, 1, T]\n",
        "  paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
        "  outputs = tf.where(key_masks, outputs, paddings)  # [B, N, 1, T]\n",
        "\n",
        "  # Scale\n",
        "  outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "  # Activation\n",
        "  outputs = tf.nn.softmax(outputs)  # [B, N, 1, T]\n",
        "  outputs = tf.reshape(outputs, [-1, 1, max_len])\n",
        "  keys = tf.reshape(keys, [-1, max_len, queries_hidden_units])\n",
        "  #print outputs.get_shape().as_list()\n",
        "  #print keys.get_sahpe().as_list()\n",
        "  # Weighted sum\n",
        "  outputs = tf.matmul(outputs, keys)\n",
        "  outputs = tf.reshape(outputs, [-1, queries_nums, queries_hidden_units])  # [B, N, 1, H]\n",
        "  print(outputs.get_shape().as_list())\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kj0Zrg3s1PA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU6GfsK9K9Zt"
      },
      "source": [
        "##Model Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxv5zCrRs_zP",
        "outputId": "62be5f4e-7d95-4f42-d51a-f8ed6efe8282"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "train_batch_size = 32\n",
        "test_batch_size = 512\n",
        "predict_batch_size = 32\n",
        "predict_users_num = 1000\n",
        "predict_ads_num = 100\n",
        "\n",
        "with open('/content/drive/MyDrive/amz_dataset/dataset.pkl', 'rb') as f:\n",
        "  train_set = pickle.load(f)\n",
        "  test_set = pickle.load(f)\n",
        "  cate_list = pickle.load(f)\n",
        "  user_count, item_count, cate_count = pickle.load(f)\n",
        "\n",
        "best_auc = 0.0\n",
        "def calc_auc(raw_arr):\n",
        "    arr = sorted(raw_arr, key=lambda d:d[2])\n",
        "\n",
        "    auc = 0.0\n",
        "    fp1, tp1, fp2, tp2 = 0.0, 0.0, 0.0, 0.0\n",
        "    for record in arr:\n",
        "        fp2 += record[0] # noclick\n",
        "        tp2 += record[1] # click\n",
        "        auc += (fp2 - fp1) * (tp2 + tp1)\n",
        "        fp1, tp1 = fp2, tp2\n",
        "\n",
        "    # if all nonclick or click, disgard\n",
        "    threshold = len(arr) - 1e-3\n",
        "    if tp2 > threshold or fp2 > threshold:\n",
        "        return -0.5\n",
        "\n",
        "    if tp2 * fp2 > 0.0:  # normal auc\n",
        "        return (1.0 - auc / (2.0 * tp2 * fp2))\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def _auc_arr(score):\n",
        "  score_p = score[:,0]\n",
        "  score_n = score[:,1]\n",
        "  score_arr = []\n",
        "  for s in score_p.tolist():\n",
        "    score_arr.append([0, 1, s])\n",
        "  for s in score_n.tolist():\n",
        "    score_arr.append([1, 0, s])\n",
        "  return score_arr\n",
        "  \n",
        "def _eval(sess, model):\n",
        "  auc_sum = 0.0\n",
        "  score_arr = []\n",
        "  # pp=\n",
        "  for _,uij in DataInputTest(test_set, test_batch_size):\n",
        "    # _, uij=pp.next()\n",
        "    auc_, score_ = model.eval(sess, uij)\n",
        "    score_arr += _auc_arr(score_)\n",
        "    auc_sum += auc_ * len(uij[0])\n",
        "  test_gauc = auc_sum / len(test_set)\n",
        "  Auc = calc_auc(score_arr)\n",
        "  global best_auc\n",
        "  if best_auc < test_gauc:\n",
        "    best_auc = test_gauc\n",
        "    model.save(sess, 'save_path/ckpt')\n",
        "  return test_gauc, Auc\n",
        "\n",
        "def _test(sess, model):\n",
        "  auc_sum = 0.0\n",
        "  score_arr = []\n",
        "  predicted_users_num = 0\n",
        "  print(\"test sub items\")\n",
        "  for _, uij in DataInputTest(test_set, predict_batch_size):\n",
        "    if predicted_users_num >= predict_users_num:\n",
        "        break\n",
        "    score_ = model.test(sess, uij)\n",
        "    score_arr.append(score_)\n",
        "    predicted_users_num += predict_batch_size\n",
        "  return score_[0]\n",
        "\n",
        "gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
        "\n",
        "  model = Model(user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.local_variables_initializer())\n",
        "\n",
        "  print('test_gauc: %.4f\\t test_auc: %.4f' % _eval(sess, model))\n",
        "  sys.stdout.flush()\n",
        "  lr = 1.0\n",
        "  start_time = time.time()\n",
        "  for _ in range(1):\n",
        "\n",
        "    random.shuffle(train_set)\n",
        "\n",
        "    epoch_size = round(len(train_set) / train_batch_size)\n",
        "    loss_sum = 0.0\n",
        "    for _, uij in DataInput(train_set, train_batch_size):\n",
        "      loss = model.train(sess, uij, lr)\n",
        "      loss_sum += loss\n",
        "\n",
        "      if model.global_step.eval() % 1000 == 0:\n",
        "        test_gauc, Auc = _eval(sess, model)\n",
        "        print('Epoch %d\\tTrain_loss: %.4f\\tEval_AUC: %.4f' %\n",
        "              (model.global_step.eval()/1000,\n",
        "               loss_sum / 1000, Auc))\n",
        "        sys.stdout.flush()\n",
        "        loss_sum = 0.0\n",
        "\n",
        "      if model.global_step.eval() % 336000 == 0:\n",
        "        lr = 0.1\n",
        "\n",
        "    print('Epochs DONE\\tCost time: %.2f' %\n",
        "          (model.global_epoch_step.eval(), time.time()-start_time))\n",
        "    sys.stdout.flush()\n",
        "    model.global_epoch_step_op.eval()\n",
        "\n",
        "  print('best test_auc:', best_auc)\n",
        "  sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-9f67b3553231>:205: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-9f67b3553231>:214: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-3-9f67b3553231>:46: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "[None, 128]\n",
            "[None, 128]\n",
            "[None, 128]\n",
            "[None, 128]\n",
            "[None, 100, 128]\n",
            "WARNING:tensorflow:From <ipython-input-3-9f67b3553231>:122: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "[None, 2]\n",
            "test_gauc: 0.5178\t test_auc: 0.5156\n",
            "Epoch 1\tTrain_loss: 0.6964\tEval_AUC: 0.6758\n",
            "Epoch 2\tTrain_loss: 0.6853\tEval_AUC: 0.6998\n",
            "Epoch 3\tTrain_loss: 0.6790\tEval_AUC: 0.7151\n",
            "Epoch 4\tTrain_loss: 0.6743\tEval_AUC: 0.7252\n",
            "Epoch 5\tTrain_loss: 0.6693\tEval_AUC: 0.7318\n",
            "Epoch 6\tTrain_loss: 0.6658\tEval_AUC: 0.7373\n",
            "Epoch 7\tTrain_loss: 0.6609\tEval_AUC: 0.7423\n",
            "Epoch 8\tTrain_loss: 0.6589\tEval_AUC: 0.7454\n",
            "Epoch 9\tTrain_loss: 0.6547\tEval_AUC: 0.7488\n",
            "Epoch 10\tTrain_loss: 0.6522\tEval_AUC: 0.7510\n",
            "Epoch 11\tTrain_loss: 0.6488\tEval_AUC: 0.7530\n",
            "Epoch 12\tTrain_loss: 0.6446\tEval_AUC: 0.7553\n",
            "Epoch 13\tTrain_loss: 0.6438\tEval_AUC: 0.7569\n",
            "Epoch 14\tTrain_loss: 0.6404\tEval_AUC: 0.7581\n",
            "Epoch 15\tTrain_loss: 0.6396\tEval_AUC: 0.7592\n",
            "Epoch 16\tTrain_loss: 0.6371\tEval_AUC: 0.7599\n",
            "Epoch 17\tTrain_loss: 0.6354\tEval_AUC: 0.7610\n",
            "Epoch 18\tTrain_loss: 0.6316\tEval_AUC: 0.7613\n",
            "Epoch 19\tTrain_loss: 0.6292\tEval_AUC: 0.7621\n",
            "Epoch 20\tTrain_loss: 0.6270\tEval_AUC: 0.7631\n",
            "Epoch 21\tTrain_loss: 0.6270\tEval_AUC: 0.7641\n",
            "Epoch 22\tTrain_loss: 0.6254\tEval_AUC: 0.7648\n",
            "Epoch 23\tTrain_loss: 0.6214\tEval_AUC: 0.7650\n",
            "Epoch 24\tTrain_loss: 0.6208\tEval_AUC: 0.7657\n",
            "Epoch 25\tTrain_loss: 0.6192\tEval_AUC: 0.7659\n",
            "Epoch 26\tTrain_loss: 0.6195\tEval_AUC: 0.7662\n",
            "Epoch 27\tTrain_loss: 0.6173\tEval_AUC: 0.7665\n",
            "Epoch 28\tTrain_loss: 0.6138\tEval_AUC: 0.7671\n",
            "Epoch 29\tTrain_loss: 0.6149\tEval_AUC: 0.7676\n",
            "Epoch 30\tTrain_loss: 0.6149\tEval_AUC: 0.7680\n",
            "Epoch 31\tTrain_loss: 0.6128\tEval_AUC: 0.7683\n",
            "Epoch 32\tTrain_loss: 0.6092\tEval_AUC: 0.7688\n",
            "Epoch 33\tTrain_loss: 0.6101\tEval_AUC: 0.7693\n",
            "Epoch 34\tTrain_loss: 0.6052\tEval_AUC: 0.7699\n",
            "Epoch 35\tTrain_loss: 0.6052\tEval_AUC: 0.7704\n",
            "Epoch 36\tTrain_loss: 0.6059\tEval_AUC: 0.7708\n",
            "Epoch 37\tTrain_loss: 0.6057\tEval_AUC: 0.7710\n",
            "Epoch 38\tTrain_loss: 0.6055\tEval_AUC: 0.7715\n",
            "Epoch 39\tTrain_loss: 0.6029\tEval_AUC: 0.7717\n",
            "Epoch 40\tTrain_loss: 0.6015\tEval_AUC: 0.7719\n",
            "Epoch 41\tTrain_loss: 0.6018\tEval_AUC: 0.7722\n",
            "Epoch 42\tTrain_loss: 0.6006\tEval_AUC: 0.7724\n",
            "Epoch 43\tTrain_loss: 0.5994\tEval_AUC: 0.7727\n",
            "Epoch 44\tTrain_loss: 0.5984\tEval_AUC: 0.7728\n",
            "Epoch 45\tTrain_loss: 0.5954\tEval_AUC: 0.7731\n",
            "Epoch 46\tTrain_loss: 0.5943\tEval_AUC: 0.7733\n",
            "Epoch 47\tTrain_loss: 0.5939\tEval_AUC: 0.7735\n",
            "Epoch 48\tTrain_loss: 0.5944\tEval_AUC: 0.7737\n",
            "Epoch 49\tTrain_loss: 0.5943\tEval_AUC: 0.7738\n",
            "Epoch 50\tTrain_loss: 0.5900\tEval_AUC: 0.7740\n",
            "Epoch 51\tTrain_loss: 0.5918\tEval_AUC: 0.7742\n",
            "Epoch 52\tTrain_loss: 0.5904\tEval_AUC: 0.7744\n",
            "Epoch 53\tTrain_loss: 0.5900\tEval_AUC: 0.7745\n",
            "Epoch 54\tTrain_loss: 0.5886\tEval_AUC: 0.7748\n",
            "Epoch 55\tTrain_loss: 0.5875\tEval_AUC: 0.7751\n",
            "Epoch 56\tTrain_loss: 0.5869\tEval_AUC: 0.7752\n",
            "Epoch 57\tTrain_loss: 0.5885\tEval_AUC: 0.7753\n",
            "Epoch 58\tTrain_loss: 0.5859\tEval_AUC: 0.7754\n",
            "Epoch 59\tTrain_loss: 0.5853\tEval_AUC: 0.7756\n",
            "Epoch 60\tTrain_loss: 0.5836\tEval_AUC: 0.7758\n",
            "Epoch 61\tTrain_loss: 0.5857\tEval_AUC: 0.7759\n",
            "Epoch 62\tTrain_loss: 0.5832\tEval_AUC: 0.7761\n",
            "Epoch 63\tTrain_loss: 0.5838\tEval_AUC: 0.7762\n",
            "Epoch 64\tTrain_loss: 0.5834\tEval_AUC: 0.7763\n",
            "Epoch 65\tTrain_loss: 0.5829\tEval_AUC: 0.7765\n",
            "Epoch 66\tTrain_loss: 0.5779\tEval_AUC: 0.7766\n",
            "Epoch 67\tTrain_loss: 0.5800\tEval_AUC: 0.7766\n",
            "Epoch 68\tTrain_loss: 0.5834\tEval_AUC: 0.7767\n",
            "Epoch 69\tTrain_loss: 0.5782\tEval_AUC: 0.7768\n",
            "Epoch 70\tTrain_loss: 0.5802\tEval_AUC: 0.7769\n",
            "Epoch 71\tTrain_loss: 0.5792\tEval_AUC: 0.7768\n",
            "Epoch 72\tTrain_loss: 0.5747\tEval_AUC: 0.7759\n",
            "Epoch 73\tTrain_loss: 0.5758\tEval_AUC: 0.7755\n",
            "Epoch 74\tTrain_loss: 0.5769\tEval_AUC: 0.7749\n",
            "Epoch 75\tTrain_loss: 0.5749\tEval_AUC: 0.7732\n",
            "Epoch 76\tTrain_loss: 0.5729\tEval_AUC: 0.7718\n",
            "Epoch 77\tTrain_loss: 0.5744\tEval_AUC: 0.7733\n",
            "Epoch 78\tTrain_loss: 0.5712\tEval_AUC: 0.7733\n",
            "Epoch 79\tTrain_loss: 0.5733\tEval_AUC: 0.7730\n",
            "Epoch 80\tTrain_loss: 0.5716\tEval_AUC: 0.7708\n",
            "Epoch 81\tTrain_loss: 0.5723\tEval_AUC: 0.7734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmQymPFtMM1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ed1292-5cae-4d26-a238-2c3c9e71c63b"
      },
      "source": [
        "print('best test_auc:', best_auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best test_auc: 0.7765211560540006\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}