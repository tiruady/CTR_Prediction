{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mfHF4RKOJMj"
      },
      "source": [
        "##Input functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkP9i-kQARRL"
      },
      "source": [
        "#input\n",
        "import numpy as np\n",
        "\n",
        "class DataInput:\n",
        "  def __init__(self, data, batch_size):\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.data = data\n",
        "    self.epoch_size = len(self.data) // self .batch_size\n",
        "    if self.epoch_size * self.batch_size < len(self.data):\n",
        "      self.epoch_size += 1\n",
        "    self.i = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "\n",
        "    if self.i == self.epoch_size:\n",
        "      raise StopIteration\n",
        "\n",
        "    ts = self.data[self.i * self.batch_size : min((self.i+1) * self.batch_size,\n",
        "                                                  len(self.data))]\n",
        "    self.i += 1\n",
        "\n",
        "    u, i, y, sl = [], [], [], []\n",
        "    for t in ts:\n",
        "      u.append(t[0])\n",
        "      i.append(t[2])\n",
        "      y.append(t[3])\n",
        "      sl.append(len(t[1]))\n",
        "    max_sl = max(sl)\n",
        "\n",
        "    hist_i = np.zeros([len(ts), max_sl], np.int64)\n",
        "\n",
        "    k = 0\n",
        "    for t in ts:\n",
        "      for l in range(len(t[1])):\n",
        "        hist_i[k][l] = t[1][l]\n",
        "      k += 1\n",
        "\n",
        "    return self.i, (u, i, y, hist_i, sl)\n",
        "\n",
        "class DataInputTest:\n",
        "  def __init__(self, data, batch_size):\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.data = data\n",
        "    self.epoch_size = len(self.data) // self.batch_size\n",
        "    if self.epoch_size * self.batch_size < len(self.data):\n",
        "      self.epoch_size += 1\n",
        "    self.i = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "\n",
        "    if self.i == self.epoch_size:\n",
        "      raise StopIteration\n",
        "\n",
        "    ts = self.data[self.i * self.batch_size : min((self.i+1) * self.batch_size,\n",
        "                                                  len(self.data))]\n",
        "    self.i += 1\n",
        "\n",
        "    u, i, j, sl = [], [], [], []\n",
        "    for t in ts:\n",
        "      u.append(t[0])\n",
        "      i.append(t[2][0])\n",
        "      j.append(t[2][1])\n",
        "      sl.append(len(t[1]))\n",
        "    max_sl = max(sl)\n",
        "\n",
        "    hist_i = np.zeros([len(ts), max_sl], np.int64)\n",
        "\n",
        "    k = 0\n",
        "    for t in ts:\n",
        "      for l in range(len(t[1])):\n",
        "        hist_i[k][l] = t[1][l]\n",
        "      k += 1\n",
        "\n",
        "    return self.i, (u, i, j, hist_i, sl)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbUZVqZbONs2"
      },
      "source": [
        "##DeepFM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98DdQdLzMpvw"
      },
      "source": [
        "#deepfm\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "class Model(object):\n",
        "\n",
        "  def __init__(self, user_count, item_count, cate_count, cate_list):\n",
        "\n",
        "    self.u = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.i = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.j = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.y = tf.placeholder(tf.float32, [None,]) # [B]\n",
        "    self.hist_i = tf.placeholder(tf.int32, [None, None]) # [B, T]\n",
        "    self.sl = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.lr = tf.placeholder(tf.float64, [])\n",
        "\n",
        "    hidden_units = 128\n",
        "\n",
        "    user_emb_w = tf.get_variable(\"user_emb_w\", [user_count, hidden_units])\n",
        "    item_emb_w = tf.get_variable(\"item_emb_w\", [item_count, hidden_units // 2])\n",
        "    item_b = tf.get_variable(\"item_b\", [item_count],\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "    cate_emb_w = tf.get_variable(\"cate_emb_w\", [cate_count, hidden_units // 2])\n",
        "    cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\n",
        "\n",
        "    u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)\n",
        "\n",
        "    ic = tf.gather(cate_list, self.i)\n",
        "    i_emb = tf.concat(values = [\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, ic),\n",
        "        ], axis=1)\n",
        "    i_b = tf.gather(item_b, self.i)\n",
        "\n",
        "    jc = tf.gather(cate_list, self.j)\n",
        "    j_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.j),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, jc),\n",
        "        ], axis=1)\n",
        "    j_b = tf.gather(item_b, self.j)\n",
        "\n",
        "    hc = tf.gather(cate_list, self.hist_i)\n",
        "    h_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.hist_i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, hc),\n",
        "        ], axis=2)\n",
        "    #-- sum begin --------\n",
        "    # mask the zero padding part\n",
        "    mask = tf.sequence_mask(self.sl, tf.shape(h_emb)[1], dtype=tf.float32) # [B, T]\n",
        "    mask = tf.expand_dims(mask, -1) # [B, T, 1]\n",
        "    mask = tf.tile(mask, [1, 1, tf.shape(h_emb)[2]]) # [B, T, H]\n",
        "    h_emb *= mask # [B, T, H]\n",
        "    hist = h_emb\n",
        "    hist = tf.reduce_sum(hist, 1) \n",
        "    hist = tf.div(hist, tf.cast(tf.tile(tf.expand_dims(self.sl,1), [1,128]), tf.float32))\n",
        "    print(h_emb.get_shape().as_list())\n",
        "    #-- sum end ---------\n",
        "    \n",
        "    hist = tf.layers.batch_normalization(inputs = hist)\n",
        "    hist = tf.reshape(hist, [-1, hidden_units])\n",
        "    hist = tf.layers.dense(hist, hidden_units)\n",
        "\n",
        "    u_emb = hist\n",
        "    #-- fcn begin -------\n",
        "    din_i = tf.concat([u_emb, i_emb], axis=-1)\n",
        "    din_i = tf.layers.batch_normalization(inputs=din_i, name='b1')\n",
        "    d_layer_1_i = tf.layers.dense(din_i, 80, activation=tf.nn.sigmoid, name='f1')\n",
        "    d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=tf.nn.sigmoid, name='f2')\n",
        "    d_layer_3_i = tf.layers.dense(d_layer_2_i, 1, activation=None, name='f3')\n",
        "    # fm part\n",
        "    d_layer_fm_i = tf.concat([tf.reduce_sum(u_emb*i_emb, axis=-1, keep_dims=True), tf.gather(u_emb, [0], axis=-1) + tf.gather(i_emb, [0], axis=-1)], axis=-1)\n",
        "    d_layer_fm_i = tf.layers.dense(d_layer_fm_i, 1, activation=None, name='f_fm')\n",
        "    din_j = tf.concat([u_emb, j_emb], axis=-1)\n",
        "    din_j = tf.layers.batch_normalization(inputs=din_j, name='b1', reuse=True)\n",
        "    d_layer_1_j = tf.layers.dense(din_j, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
        "    d_layer_2_j = tf.layers.dense(d_layer_1_j, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
        "    d_layer_3_j = tf.layers.dense(d_layer_2_j, 1, activation=None, name='f3', reuse=True)\n",
        "    d_layer_fm_j = tf.concat([tf.reduce_sum(u_emb*j_emb, axis=-1, keep_dims=True), tf.gather(u_emb, [0], axis=-1) + tf.gather(j_emb, [0], axis=-1)], axis=-1)\n",
        "    d_layer_fm_j = tf.layers.dense(d_layer_fm_j, 1, activation=None, name='f_fm', reuse=True)\n",
        "    d_layer_3_i = tf.reshape(d_layer_3_i, [-1])\n",
        "    d_layer_3_j = tf.reshape(d_layer_3_j, [-1])\n",
        "    d_layer_fm_i = tf.reshape(d_layer_fm_i, [-1])\n",
        "    d_layer_fm_j = tf.reshape(d_layer_fm_j, [-1])\n",
        "    x = i_b - j_b + d_layer_3_i - d_layer_3_j + d_layer_fm_i - d_layer_fm_j # [B]\n",
        "    self.logits = i_b + d_layer_3_i + d_layer_fm_i\n",
        "    u_emb_all = tf.expand_dims(u_emb, 1)\n",
        "    u_emb_all = tf.tile(u_emb_all, [1, item_count, 1])\n",
        "    # logits for all item:\n",
        "    all_emb = tf.concat([\n",
        "        item_emb_w,\n",
        "        tf.nn.embedding_lookup(cate_emb_w, cate_list)\n",
        "        ], axis=1)\n",
        "    all_emb = tf.expand_dims(all_emb, 0)\n",
        "    all_emb = tf.tile(all_emb, [512, 1, 1])\n",
        "    din_all = tf.concat([u_emb_all, all_emb], axis=-1)\n",
        "    din_all = tf.layers.batch_normalization(inputs=din_all, name='b1', reuse=True)\n",
        "    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
        "    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
        "    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3', reuse=True)\n",
        "    d_layer_fm_all = tf.concat([tf.reduce_sum(u_emb_all*all_emb, axis=-1, keep_dims=True), tf.gather(u_emb_all, [0], axis=-1) + tf.gather(all_emb, [0], axis=-1)], axis=-1)\n",
        "    d_layer_fm_all = tf.layers.dense(d_layer_fm_all, 1, activation=None, name='f_fm', reuse=True)\n",
        "    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, item_count])\n",
        "    d_layer_fm_all = tf.reshape(d_layer_fm_all, [-1, item_count])\n",
        "    self.logits_all = tf.sigmoid(item_b + d_layer_3_all + d_layer_fm_all)\n",
        "\n",
        "    \n",
        "    self.mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
        "    self.score_i = tf.sigmoid(i_b + d_layer_3_i + d_layer_fm_i)\n",
        "    self.score_j = tf.sigmoid(j_b + d_layer_3_j + d_layer_fm_j)\n",
        "    self.score_i = tf.reshape(self.score_i, [-1, 1])\n",
        "    self.score_j = tf.reshape(self.score_j, [-1, 1])\n",
        "    self.p_and_n = tf.concat([self.score_i, self.score_j], axis=-1)\n",
        "    print(self.p_and_n.get_shape().as_list())\n",
        "    self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "    self.global_epoch_step = \\\n",
        "        tf.Variable(0, trainable=False, name='global_epoch_step')\n",
        "    self.global_epoch_step_op = \\\n",
        "        tf.assign(self.global_epoch_step, self.global_epoch_step+1)\n",
        "\n",
        "    regulation_rate = 0.0\n",
        "    self.loss = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=self.logits,\n",
        "            labels=self.y)\n",
        "        )\n",
        "\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    self.opt = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "\n",
        "    gradients = tf.gradients(self.loss, trainable_params)\n",
        "    clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
        "    self.train_op = self.opt.apply_gradients(\n",
        "        zip(clip_gradients, trainable_params), global_step=self.global_step)\n",
        "\n",
        "\n",
        "  def train(self, sess, uij, l):\n",
        "    loss, _ = sess.run([self.loss, self.train_op], feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.y: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        self.lr: l,\n",
        "        })\n",
        "    return loss\n",
        "\n",
        "  def eval(self, sess, uij):\n",
        "    u_auc, socre_p_and_n = sess.run([self.mf_auc, self.p_and_n], feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.j: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        })\n",
        "    return u_auc, socre_p_and_n\n",
        "\n",
        "  def test(self, sess, uid, hist_i, sl):\n",
        "    return sess.run(self.logits_all, feed_dict={\n",
        "        self.u: uid,\n",
        "        self.hist_i: hist_i,\n",
        "        self.sl: sl,\n",
        "        })\n",
        "\n",
        "  def save(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path=path)\n",
        "\n",
        "  def restore(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, save_path=path)\n",
        "\n",
        "def extract_axis_1(data, ind):\n",
        "  batch_range = tf.range(tf.shape(data)[0])\n",
        "  indices = tf.stack([batch_range, ind], axis=1)\n",
        "  res = tf.gather_nd(data, indices)\n",
        "  return res"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pix4raP8OSCI"
      },
      "source": [
        "##Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TAGuU7fxpiq",
        "outputId": "1a20f31a-76cb-4ed5-be29-69cdfab4b075"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "train_batch_size = 32\n",
        "test_batch_size = 512\n",
        "\n",
        "with open('/content/drive/MyDrive/amz_dataset/dataset.pkl', 'rb') as f:\n",
        "  train_set = pickle.load(f)\n",
        "  test_set = pickle.load(f)\n",
        "  cate_list = pickle.load(f)\n",
        "  user_count, item_count, cate_count = pickle.load(f)\n",
        "\n",
        "best_auc = 0.0\n",
        "def calc_auc(raw_arr):\n",
        "    arr = sorted(raw_arr, key=lambda d:d[2])\n",
        "\n",
        "    auc = 0.0\n",
        "    fp1, tp1, fp2, tp2 = 0.0, 0.0, 0.0, 0.0\n",
        "    for record in arr:\n",
        "        fp2 += record[0] # noclick\n",
        "        tp2 += record[1] # click\n",
        "        auc += (fp2 - fp1) * (tp2 + tp1)\n",
        "        fp1, tp1 = fp2, tp2\n",
        "\n",
        "    threshold = len(arr) - 1e-3\n",
        "    if tp2 > threshold or fp2 > threshold:\n",
        "        return -0.5\n",
        "\n",
        "    if tp2 * fp2 > 0.0: \n",
        "        return (1.0 - auc / (2.0 * tp2 * fp2))\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def _auc_arr(score):\n",
        "  score_p = score[:,0]\n",
        "  score_n = score[:,1]\n",
        "  score_arr = []\n",
        "  for s in score_p.tolist():\n",
        "    score_arr.append([0, 1, s])\n",
        "  for s in score_n.tolist():\n",
        "    score_arr.append([1, 0, s])\n",
        "  return score_arr\n",
        "def _eval(sess, model):\n",
        "  auc_sum = 0.0\n",
        "  score_arr = []\n",
        "  for _, uij in DataInputTest(test_set, test_batch_size):\n",
        "    auc_, score_ = model.eval(sess, uij)\n",
        "    score_arr += _auc_arr(score_)\n",
        "    auc_sum += auc_ * len(uij[0])\n",
        "  test_gauc = auc_sum / len(test_set)\n",
        "  Auc = calc_auc(score_arr)\n",
        "  global best_auc\n",
        "  if best_auc < test_gauc:\n",
        "    best_auc = test_gauc\n",
        "    model.save(sess, 'save_path/ckpt')\n",
        "  return test_gauc, Auc\n",
        "\n",
        "\n",
        "gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
        "\n",
        "  model = Model(user_count, item_count, cate_count, cate_list)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.local_variables_initializer())\n",
        "\n",
        "  print('test_gauc: %.4f\\t test_auc: %.4f' % _eval(sess, model))\n",
        "  sys.stdout.flush()\n",
        "  lr = 0.001\n",
        "  start_time = time.time()\n",
        "  for _ in range(1):\n",
        "\n",
        "    random.shuffle(train_set)\n",
        "\n",
        "    epoch_size = round(len(train_set) / train_batch_size)\n",
        "    loss_sum = 0.0\n",
        "    for _, uij in DataInput(train_set, train_batch_size):\n",
        "      loss = model.train(sess, uij, lr)\n",
        "      loss_sum += loss\n",
        "\n",
        "      if model.global_step.eval() % 1000 == 0:\n",
        "        test_gauc, Auc = _eval(sess, model)\n",
        "        print('Epoch %d\\tTrain_loss: %.4f\\tEval_AUC: %.4f' %\n",
        "              (model.global_step.eval()/1000,\n",
        "               loss_sum / 1000, Auc))\n",
        "        sys.stdout.flush()\n",
        "        loss_sum = 0.0\n",
        "\n",
        "      if model.global_step.eval() % 336000 == 0:\n",
        "        lr *= 0.9\n",
        "\n",
        "    print('Epochs DONE\\tCost time: %.2f' %\n",
        "          ( time.time()-start_time))\n",
        "    sys.stdout.flush()\n",
        "    model.global_epoch_step_op.eval()\n",
        "\n",
        "  print('best test_auc:', best_auc)\n",
        "  sys.stdout.flush()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-477b2e20deb4>:54: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "[None, None, 128]\n",
            "WARNING:tensorflow:From <ipython-input-5-477b2e20deb4>:58: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-477b2e20deb4>:60: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-477b2e20deb4>:70: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-5-477b2e20deb4>:107: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "[None, 2]\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "test_gauc: 0.5011\t test_auc: 0.5003\n",
            "Epoch 1\tTrain_loss: 0.6517\tEval_AUC: 0.6929\n",
            "Epoch 2\tTrain_loss: 0.6017\tEval_AUC: 0.7495\n",
            "Epoch 3\tTrain_loss: 0.5693\tEval_AUC: 0.7688\n",
            "Epoch 4\tTrain_loss: 0.5554\tEval_AUC: 0.7853\n",
            "Epoch 5\tTrain_loss: 0.5394\tEval_AUC: 0.7923\n",
            "Epoch 6\tTrain_loss: 0.5373\tEval_AUC: 0.7914\n",
            "Epoch 7\tTrain_loss: 0.5224\tEval_AUC: 0.8016\n",
            "Epoch 8\tTrain_loss: 0.5274\tEval_AUC: 0.8068\n",
            "Epoch 9\tTrain_loss: 0.5186\tEval_AUC: 0.8101\n",
            "Epoch 10\tTrain_loss: 0.5171\tEval_AUC: 0.8141\n",
            "Epoch 11\tTrain_loss: 0.5090\tEval_AUC: 0.8154\n",
            "Epoch 12\tTrain_loss: 0.5048\tEval_AUC: 0.8183\n",
            "Epoch 13\tTrain_loss: 0.5038\tEval_AUC: 0.8208\n",
            "Epoch 14\tTrain_loss: 0.5029\tEval_AUC: 0.8223\n",
            "Epoch 15\tTrain_loss: 0.5004\tEval_AUC: 0.8214\n",
            "Epoch 16\tTrain_loss: 0.4978\tEval_AUC: 0.8266\n",
            "Epoch 17\tTrain_loss: 0.4964\tEval_AUC: 0.8238\n",
            "Epoch 18\tTrain_loss: 0.4915\tEval_AUC: 0.8275\n",
            "Epoch 19\tTrain_loss: 0.4893\tEval_AUC: 0.8271\n",
            "Epoch 20\tTrain_loss: 0.4810\tEval_AUC: 0.8312\n",
            "Epoch 21\tTrain_loss: 0.4877\tEval_AUC: 0.8290\n",
            "Epoch 22\tTrain_loss: 0.4824\tEval_AUC: 0.8313\n",
            "Epoch 23\tTrain_loss: 0.4823\tEval_AUC: 0.8334\n",
            "Epoch 24\tTrain_loss: 0.4804\tEval_AUC: 0.8337\n",
            "Epoch 25\tTrain_loss: 0.4765\tEval_AUC: 0.8351\n",
            "Epoch 26\tTrain_loss: 0.4832\tEval_AUC: 0.8366\n",
            "Epoch 27\tTrain_loss: 0.4734\tEval_AUC: 0.8362\n",
            "Epoch 28\tTrain_loss: 0.4714\tEval_AUC: 0.8391\n",
            "Epoch 29\tTrain_loss: 0.4740\tEval_AUC: 0.8389\n",
            "Epoch 30\tTrain_loss: 0.4790\tEval_AUC: 0.8429\n",
            "Epoch 31\tTrain_loss: 0.4718\tEval_AUC: 0.8406\n",
            "Epoch 32\tTrain_loss: 0.4643\tEval_AUC: 0.8418\n",
            "Epoch 33\tTrain_loss: 0.4682\tEval_AUC: 0.8428\n",
            "Epoch 34\tTrain_loss: 0.4586\tEval_AUC: 0.8442\n",
            "Epoch 35\tTrain_loss: 0.4649\tEval_AUC: 0.8450\n",
            "Epoch 36\tTrain_loss: 0.4643\tEval_AUC: 0.8472\n",
            "Epoch 37\tTrain_loss: 0.4585\tEval_AUC: 0.8438\n",
            "Epoch 38\tTrain_loss: 0.4616\tEval_AUC: 0.8480\n",
            "Epoch 39\tTrain_loss: 0.4617\tEval_AUC: 0.8519\n",
            "Epoch 40\tTrain_loss: 0.4584\tEval_AUC: 0.8476\n",
            "Epoch 41\tTrain_loss: 0.4581\tEval_AUC: 0.8468\n",
            "Epoch 42\tTrain_loss: 0.4601\tEval_AUC: 0.8487\n",
            "Epoch 43\tTrain_loss: 0.4533\tEval_AUC: 0.8478\n",
            "Epoch 44\tTrain_loss: 0.4566\tEval_AUC: 0.8542\n",
            "Epoch 45\tTrain_loss: 0.4478\tEval_AUC: 0.8513\n",
            "Epoch 46\tTrain_loss: 0.4517\tEval_AUC: 0.8523\n",
            "Epoch 47\tTrain_loss: 0.4538\tEval_AUC: 0.8529\n",
            "Epoch 48\tTrain_loss: 0.4518\tEval_AUC: 0.8543\n",
            "Epoch 49\tTrain_loss: 0.4527\tEval_AUC: 0.8559\n",
            "Epoch 50\tTrain_loss: 0.4478\tEval_AUC: 0.8540\n",
            "Epoch 51\tTrain_loss: 0.4475\tEval_AUC: 0.8555\n",
            "Epoch 52\tTrain_loss: 0.4420\tEval_AUC: 0.8549\n",
            "Epoch 53\tTrain_loss: 0.4459\tEval_AUC: 0.8546\n",
            "Epoch 54\tTrain_loss: 0.4493\tEval_AUC: 0.8563\n",
            "Epoch 55\tTrain_loss: 0.4442\tEval_AUC: 0.8573\n",
            "Epoch 56\tTrain_loss: 0.4405\tEval_AUC: 0.8582\n",
            "Epoch 57\tTrain_loss: 0.4462\tEval_AUC: 0.8582\n",
            "Epoch 58\tTrain_loss: 0.4437\tEval_AUC: 0.8594\n",
            "Epoch 59\tTrain_loss: 0.4413\tEval_AUC: 0.8561\n",
            "Epoch 60\tTrain_loss: 0.4397\tEval_AUC: 0.8608\n",
            "Epoch 61\tTrain_loss: 0.4447\tEval_AUC: 0.8613\n",
            "Epoch 62\tTrain_loss: 0.4399\tEval_AUC: 0.8593\n",
            "Epoch 63\tTrain_loss: 0.4429\tEval_AUC: 0.8586\n",
            "Epoch 64\tTrain_loss: 0.4380\tEval_AUC: 0.8616\n",
            "Epoch 65\tTrain_loss: 0.4397\tEval_AUC: 0.8624\n",
            "Epoch 66\tTrain_loss: 0.4338\tEval_AUC: 0.8596\n",
            "Epoch 67\tTrain_loss: 0.4354\tEval_AUC: 0.8580\n",
            "Epoch 68\tTrain_loss: 0.4420\tEval_AUC: 0.8619\n",
            "Epoch 69\tTrain_loss: 0.4336\tEval_AUC: 0.8661\n",
            "Epoch 70\tTrain_loss: 0.4428\tEval_AUC: 0.8639\n",
            "Epoch 71\tTrain_loss: 0.4376\tEval_AUC: 0.8619\n",
            "Epoch 72\tTrain_loss: 0.4347\tEval_AUC: 0.8619\n",
            "Epoch 73\tTrain_loss: 0.4383\tEval_AUC: 0.8625\n",
            "Epoch 74\tTrain_loss: 0.4366\tEval_AUC: 0.8629\n",
            "Epoch 75\tTrain_loss: 0.4335\tEval_AUC: 0.8610\n",
            "Epoch 76\tTrain_loss: 0.4313\tEval_AUC: 0.8639\n",
            "Epoch 77\tTrain_loss: 0.4342\tEval_AUC: 0.8652\n",
            "Epoch 78\tTrain_loss: 0.4267\tEval_AUC: 0.8653\n",
            "Epoch 79\tTrain_loss: 0.4309\tEval_AUC: 0.8642\n",
            "Epoch 80\tTrain_loss: 0.4301\tEval_AUC: 0.8632\n",
            "Epochs DONE\tCost time: 3876.28\n",
            "best test_auc: 0.8650644740181618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05qg0bQFyiyB",
        "outputId": "cc5620c9-23c3-401f-85d9-c7dffc4bd48b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k7n_B1SyjVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}